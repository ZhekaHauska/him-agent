log: True
project_name: 'sp_attractor'
run_tag: null

n_episodes: 1001
update_rate: 200
max_steps: 100

# testing
attractor_steps: 20
n_trajectories: 500
learn_attractor_in_loop: False
pairs_per_trajectory: 10
# som
som_iterations: 1000
som_learning_rate: 0.1
som_sigma: 1.0
som_size: 100

encoder_conf: 'configs/sp/encoder.yaml'
attractor_conf: 'configs/sp/attractor.yaml'

experiment: mnist

seed: null

rbits:
  log: True
  project_name: 'sp_attractor_rbits'
  run_tag: null

  n_episodes: 1001
  update_rate: 200
  max_steps: 100

  # testing
  attractor_steps: 20
  n_trajectories: 500
  learn_attractor_in_loop: True
  pairs_per_trajectory: 1
  # som
  som_iterations: 1000
  som_learning_rate: 0.1
  som_sigma: 1.0
  som_size: 100

  encoder_conf: 'configs/sp/encoder.yaml'
  attractor_conf: 'configs/sp/attractor.yaml'
  env_conf: 'configs/rbits/default.yaml'

  experiment: rbits

  seed: null

rbits_default:
  sds: [ 64, 0.375 ]
  similarity_range: [ 0, 1 ]

sp:
  default:
    seed: null
    # input
    feedforward_sds: null
    # начальный размер рецептивного поля (в единицах среднего размера активного входа)
    initial_rf_to_input_ratio: 8.0
    # конечный размер рецептивного поля (-//-)
    max_rf_to_input_ratio: 1.5
    # output
    output_sds: null
    # learning
    min_overlap_for_activation: 3
    learning_rate: 0.02
    # ослабление весов здесь задается относительно learning_rate (поэтому этот параметр обычно не нужно править)
    global_inhibition_strength: 0.2

    # далее настройки новорожденной фазы:
    # частота прунинга (в единицах размера всего выхода SP)
    newborn_pruning_cycle: 2.0
    # сколько прунингов делается перед взрослением
    newborn_pruning_stages: 10
    # нужно ли вести статистику активного размера входа (ставь True, если вход имеет непостоянный сильно варьирующийся размер или неизвестен — это важно, тк у нас размеры рецептивных полей завязаны на значение активного размера входа)
    adapt_to_ff_sparsity: False
    # 0 -> бустинг выключен, если больше -> включен. Работает только в новорожденной фазе
    boosting_k: 0.5

  encoder:
    seed: ???
    # input
    feedforward_sds: ???
    # начальный размер рецептивного поля (в единицах среднего размера активного входа)
    initial_rf_to_input_ratio: 8.0
    # конечный размер рецептивного поля (-//-)
    max_rf_to_input_ratio: 1.5
    # это максимальный конечный размер рецептивного поля, он дополняет max_rf_to_input_ratio, тк этот уже в абсолютных величинах sparsity.
    # В итоге берется минимум из этих двух параметров.
    max_rf_sparsity: 0.1
    # output
    output_sds: [ 1000, 0.02 ]
    # learning
    min_overlap_for_activation: 3
    learning_rate: 0.02
    # ослабление весов здесь задается относительно learning_rate (поэтому этот параметр обычно не нужно править)
    global_inhibition_strength: 0.2

    # далее настройки новорожденной фазы:
    # частота прунинга (в единицах размера всего выхода SP)
    newborn_pruning_cycle: 2.0
    # Этот аналогичен newborn_pruning_cycle, только определяет частоту операции prune-grow, которая на самом деле просто полное пересэмплирование рецептивного поля нейронов с очень низкой частотой активации. (тоже в единицах размера всего выхода SP)
    prune_grow_cycle: 10.0
    # сколько прунингов делается перед взрослением
    newborn_pruning_stages: 10
    # нужно ли вести статистику активного размера входа (ставь True, если вход имеет непостоянный сильно варьирующийся размер или неизвестен — это важно, тк у нас размеры рецептивных полей завязаны на значение активного размера входа)
    adapt_to_ff_sparsity: True
    # 0 -> бустинг выключен, если больше -> включен. Работает только в новорожденной фазе
    boosting_k: 0.5

  attractor:
    seed: null
    # input
    feedforward_sds: null
    # начальный размер рецептивного поля (в единицах среднего размера активного входа)
    initial_rf_to_input_ratio: 8.0
    # конечный размер рецептивного поля (-//-)
    max_rf_to_input_ratio: 1.5
    # это максимальный конечный размер рецептивного поля, он дополняет max_rf_to_input_ratio, тк этот уже в абсолютных величинах sparsity.
    # В итоге берется минимум из этих двух параметров.
    max_rf_sparsity: 0.1
    # output
    output_sds: [ 1000, 0.02 ]
    # learning
    min_overlap_for_activation: 3
    learning_rate: 0.02
    # ослабление весов здесь задается относительно learning_rate (поэтому этот параметр обычно не нужно править)
    global_inhibition_strength: 0.2

    # далее настройки новорожденной фазы:
    # частота прунинга (в единицах размера всего выхода SP)
    newborn_pruning_cycle: 2.0
    # Этот аналогичен newborn_pruning_cycle, только определяет частоту операции prune-grow, которая на самом деле просто полное пересэмплирование рецептивного поля нейронов с очень низкой частотой активации. (тоже в единицах размера всего выхода SP)
    prune_grow_cycle: 10.0
    # сколько прунингов делается перед взрослением
    newborn_pruning_stages: 10
    # нужно ли вести статистику активного размера входа (ставь True, если вход имеет непостоянный сильно варьирующийся размер или неизвестен — это важно, тк у нас размеры рецептивных полей завязаны на значение активного размера входа)
    adapt_to_ff_sparsity: False
    # 0 -> бустинг выключен, если больше -> включен. Работает только в новорожденной фазе
    boosting_k: 0.5

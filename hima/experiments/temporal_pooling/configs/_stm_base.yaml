dataset:
  synthetic_sequences:
    base:
      _type_: ds.synthetic_sequences
      global_config: ???
      seed: ???
      # sequences
      n_sequences: ???
      sequence_length: ???
      sequence_similarity: ???
      sequence_similarity_std: ???
      # elements
      alphabet_size: ???
      encoder: ???

    # TODO: high similarity
    similar:
      _base_: base
      sequence_similarity: ???
      sequence_similarity_std: ???

    # mid similarity
    general:
      _base_: base
      sequence_similarity: 0.2
      sequence_similarity_std: 0.33

    # TODO: low similarity
    dissimilar:
      _base_: base
      sequence_similarity: ???
      sequence_similarity_std: ???

  dvc_sequences:
    _type_: ds.dvc_sequences
    filepath: ~/data/outdoors_walking/sdrs.pkl
    n_sequences: ???
    sequence_length: ???
    seed: ???
    sequential: ???

  text_sequences:
    _type_: ds.text_sequences
    global_config: ???
    seed: ???
    filepath: ~/data/text/hima_all.txt
    n_sequences: ???
    sequence_length: ???
    encoder: ???
    sequential: ???
    # hint to limit the total size while reading the dataset file in bytes/chars: 10MB
    max_size_hint: 10_000_000

encoder:
  bucket:
    _type_: encoder.int_bucket
    n_values: ???
    bucket_size: ???

  random:
    base:
      _type_: encoder.int_random
      n_values: ???
      space_compression: ???
      active_size: ???
      sds: ???
      seed: ???
      output_mode: binary
    flexible_sds:
      _base_: base
      active_size: ???
      space_compression: ???
      sds: ...
    fixed_sds:
      _base_: base
      space_compression: ...
      active_size: ...
      sds: ???

block:
  # you can omit specifying this common base as _base_.
  # it is added just for clarity that each block has these attributes
  base:
    name: ???

blocks:
  sp_like:
    output_sds: ???
    sp: ???
  tm_like:
    active_cells_sds: ???
    predicted_cells_sds: ???
    correctly_predicted_cells_sds: ???
    tm: ???

models:
  LSTM:
    - input.sdr -> TM.feedforward.sdr
    - compute LSTM:
      - TM.predict()
      - TM.set_predicted_cells()
      - TM.activate()
    - TM.correctly_predicted_cells.sdr -> output.sdr
  Baseline TM:
    - predict input w/ prior belief:
      - TM.predict()
      - TM.set_predicted_cells()
      - TM.predicted_columns.sdr -> input_prediction.sdr
    - embed input:
      - input.sdr -> embed_input.sdr
    - compute TM:
      - embed_input.sdr -> TM.feedforward.sdr
      - TM.activate()
    - predict input w/ posterior belief:
      - TM.correctly_predicted_columns.sdr -> input_posterior_prediction.sdr

spatial_pooler:
  default:
    _type_: sp.vectorized
    seed: ???
    # input
    feedforward_sds: ???
    adapt_to_ff_sparsity: False
    initial_max_rf_sparsity: 0.5
    initial_rf_to_input_ratio: 10.0
    target_max_rf_sparsity: 0.25
    target_rf_to_input_ratio: 3.0
    # output
    output_sds: ???
    # learning
    learning_rate: 0.01
    newborn_pruning_cycle: 4.0
    newborn_pruning_stages: 20
    newborn_pruning_mode: powerlaw
    prune_grow_cycle: 200.0
    boosting_k: 0.5
  default_layer:
    _type_: sp.layer
    seed: ???
    # input
    feedforward_sds: ???
    connectable_ff_size: ???
    adapt_to_ff_sparsity: False

    initial_max_rf_sparsity: 0.1
    initial_rf_to_input_ratio: 5.0
    target_max_rf_sparsity: 0.1
    target_rf_to_input_ratio: 0.5

    # learning
    learning_rate: 0.01
    learning_algo: new
    synaptogenesis_cycle: 20.0

    newborn_pruning_cycle: 10.0
    newborn_pruning_stages: 5
    newborn_pruning_mode: powerlaw
    boosting_k: 0.25

    # output
    output_sds: ???
    output_mode: binary
    sample_winners: false
    normalize_rates: true

  # MultiCompartment SP
  default_layer_mc:
    _type_: sp.layer_mc
    global_config: ???
    seed: ???

    # compartments
    compartments: ???
    compartments_config: ???
    compartments_weight: ???

    # learning
    learning_rate: 0.01
    synaptogenesis_cycle: 20.0

    # output
    output_sds: ???
    output_mode: binary

  # Compartment for MultiCompartment SP
  default_layer_compartment:
    _type_: sp.layer
    seed: ???
    # input
    feedforward_sds: ???

    rf_sparsity: 0.6
    rf_to_input_ratio: 0.75

    # learning
    learning_rate: 0.01
    learning_algo: new
    synaptogenesis_cycle: 5.0

    # output
    output_sds: ???
    output_mode: binary

  # Compartment for MultiCompartment SP
  default_layer_compartment_bckp:
    _type_: sp.layer
    seed: ???
    # input
    feedforward_sds: ???
    connectable_ff_size: ...
    adapt_to_ff_sparsity: False

    initial_rf_to_input_ratio: 100.0
    initial_max_rf_sparsity: 0.6
    target_max_rf_sparsity: 0.6
    target_rf_to_input_ratio: 0.75

    # learning
    learning_rate: 0.01
    learning_algo: new
    synaptogenesis_cycle: 5.0

    newborn_pruning_cycle: 50.0
    newborn_pruning_stages: 20
    newborn_pruning_mode: powerlaw
    boosting_k: 0.25

    # output
    output_sds: ???
    output_mode: binary
    sample_winners: false
    normalize_rates: true

temporal_memory:
  default:
    _base_: general_feedback_tm
    _type_: tm.base
  general_feedback_tm:
    _type_: tm.general_feedback
    seed: ???
    cells_per_column: ???
    activation_threshold_basal: ???
    learning_threshold_basal: ???
    max_synapses_per_segment_basal: ???
    max_segments_per_cell_basal: ???

    # Reqs
    columns: ???
    context_cells: ???
    feedback_cells: ???
    activation_threshold_apical: 1
    learning_threshold_apical: 1

decoder:
  mlp:
    _type_: decoder.mlp
    seed: ???
    feedforward_sds: ???
    output_sds: ???
    weights_scale: 0.01
    # learning_rate: 0.0002
    learning_rate: 0.0005
    # power_t: 0.1
    power_t: 0.00
    epoch_size: 1_000
    total_updates_required: 1_000_000
    collect_errors: False
    output_mode: rate
  torch_mlp:
    _base_: mlp
    _type_: decoder.mlp_torch


# additional params for wandb.init
wandb_init:
  log: ???
  project: ???
  config: ???
  name: ...
  group: ...
  # We never use resume previous run wandb option. This setting prevents warnings.
  resume: never
  # Below are params that prevent wandb doing unnecessary actions. Alter 'em when needed
  save_code: false
